{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курсовая работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет [Visual Question Answering (VQA)](https://paperswithcode.com/dataset/visual-question-answering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет Visual Question Answering (VQA) из библиотеки Facebook AI Research ParlAI представляет собой коллекцию данных, предназначенных для разработки и тестирования моделей, которые могут интерпретировать визуальные данные и отвечать на вопросы, связанные с этими данными.\n",
    "\n",
    "Каждый образец в датасете содержит изображение, вопрос на естественном языке, сформулированный относительно этого изображения, и один или несколько возможных ответов, отражающих человеческое восприятие. Вопросы охватывают широкий спектр сложностей, начиная от простых запросов, таких как «Какого цвета машина?», до более сложных, например, «Сколько людей на переднем плане и чем они занимаются?».\n",
    "\n",
    "Изображения в датасете могут быть из реального мира или сгенерированы, включать сцены с разнообразными объектами, людьми, действиями и контекстами. Визуальная информация сочетается с текстовыми компонентами, чтобы побудить модель понимать не только изолированные аспекты данных, но и их взаимосвязь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parlai.core.params import ParlaiParser\n",
    "from parlai.core.worlds import create_task\n",
    "from parlai.core.agents import create_agent\n",
    "from parlai.utils.logging import logging\n",
    "\n",
    "# Создаем парсер для параметров\n",
    "parser = ParlaiParser()\n",
    "parser.set_defaults(\n",
    "    task='vqa_v2',  # Указываем название задачи VQA v2\n",
    "    datatype='train:stream',  # Тип данных (например, обучение, тест)\n",
    "    batchsize=1,  # Размер батча (для примера установим 1)\n",
    ")\n",
    "\n",
    "# Получаем аргументы\n",
    "opt = parser.parse_args(print_args=False)\n",
    "\n",
    "# Создаем агент (например, dummy-агент для просмотра данных)\n",
    "agent = create_agent(opt)\n",
    "world = create_task(opt, agent)\n",
    "\n",
    "# Выводим пример из датасета\n",
    "print(\"Пример из датасета Visual Question Answering:\")\n",
    "for _ in range(1):  # Загружаем один пример\n",
    "    world.parley()\n",
    "    message = world.get_acts()  # Получаем сообщение из мира\n",
    "    for msg in message:\n",
    "        if 'image' in msg:\n",
    "            print(\"Изображение: доступно (бинарные данные)\")\n",
    "        if 'text' in msg:\n",
    "            print(f\"Вопрос: {msg['text']}\")\n",
    "        if 'labels' in msg:\n",
    "            print(f\"Ответы: {msg['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переменные для сбора статистики\n",
    "num_examples = 0\n",
    "question_lengths = []\n",
    "answer_counts = Counter()\n",
    "question_types = Counter()\n",
    "\n",
    "print(\"Сбор статистики по датасету VQA начат...\")\n",
    "for _ in range(10000):  # Ограничиваем до 10,000 примеров для анализа\n",
    "    world.parley()\n",
    "    message = world.get_acts()[0]  # Получаем первое сообщение из мира\n",
    "    if 'text' in message:\n",
    "        question = message['text']\n",
    "        question_lengths.append(len(question.split()))\n",
    "        question_types[question.split()[0].lower()] += 1  # Первая слово как тип вопроса\n",
    "    if 'labels' in message:\n",
    "        for label in message['labels']:\n",
    "            answer_counts[label.lower()] += 1\n",
    "    num_examples += 1\n",
    "\n",
    "print(f\"Количество примеров: {num_examples}\")\n",
    "print(f\"Средняя длина вопроса: {sum(question_lengths) / len(question_lengths):.2f}\")\n",
    "print(f\"Топ-10 наиболее частых ответов: {answer_counts.most_common(10)}\")\n",
    "print(f\"Топ-10 начальных слов вопросов: {question_types.most_common(10)}\")\n",
    "\n",
    "# Визуализация длины вопросов\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(question_lengths, bins=range(1, 21), color='skyblue', edgecolor='black')\n",
    "plt.title(\"Распределение длин вопросов\")\n",
    "plt.xlabel(\"Длина вопроса (количество слов)\")\n",
    "plt.ylabel(\"Количество вопросов\")\n",
    "plt.show()\n",
    "\n",
    "# Визуализация топ-10 ответов\n",
    "top_answers = answer_counts.most_common(10)\n",
    "answers, counts = zip(*top_answers)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(answers, counts, color='salmon')\n",
    "plt.title(\"Топ-10 наиболее частых ответов\")\n",
    "plt.xlabel(\"Ответ\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Визуализация топ-10 типов вопросов\n",
    "top_question_types = question_types.most_common(10)\n",
    "q_types, q_counts = zip(*top_question_types)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(q_types, q_counts, color='lightgreen')\n",
    "plt.title(\"Топ-10 начальных слов вопросов (типов вопросов)\")\n",
    "plt.xlabel(\"Тип вопроса (начальное слово)\")\n",
    "plt.ylabel(\"Частота\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация/понижение размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Преобразование текстовых данных в числовые векторы с помощью TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(questions)\n",
    "\n",
    "# Кластеризация вопросов с использованием KMeans\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_tfidf)\n",
    "\n",
    "# Понижение размерности для визуализации\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Визуализация кластеров\n",
    "plt.figure(figsize=(12, 8))\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_points = X_tsne[clusters == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Кластер {cluster}\", alpha=0.6)\n",
    "plt.title(\"Визуализация кластеров вопросов с помощью T-SNE\")\n",
    "plt.xlabel(\"Компонента 1\")\n",
    "plt.ylabel(\"Компонента 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нахождение выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Использование Isolation Forest для обнаружения выбросов\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)  # 2% данных считаются выбросами\n",
    "outlier_labels = iso_forest.fit_predict(X_tfidf)\n",
    "\n",
    "# Анализ выбросов\n",
    "outlier_indices = np.where(outlier_labels == -1)[0]  # Индексы выбросов\n",
    "print(f\"Найдено выбросов: {len(outlier_indices)}\")\n",
    "\n",
    "# Вывод примеров выбросов\n",
    "print(\"\\nПримеры вопросов-выбросов:\")\n",
    "for idx in outlier_indices[:10]:  # Покажем первые 10 выбросов\n",
    "    print(f\"- {questions[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Отображение данных с выделением выбросов\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c='lightblue', label='Нормальные данные', alpha=0.5)\n",
    "plt.scatter(\n",
    "    X_pca[outlier_indices, 0],\n",
    "    X_pca[outlier_indices, 1],\n",
    "    c='red',\n",
    "    label='Выбросы',\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.title(\"Выбросы в данных (на основе PCA и Isolation Forest)\")\n",
    "plt.xlabel(\"Компонента 1\")\n",
    "plt.ylabel(\"Компонента 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-systems design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет Visual Question Answering (VQA) является мощным инструментом для разработки и тестирования моделей, которые комбинируют компьютерное зрение и обработку естественного языка.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Искусственный интеллект и мультимодальные модели**\n",
    "Датасет VQA используется для обучения и оценки моделей, способных объединять визуальную и текстовую информацию. Он позволяет:\n",
    "- Разрабатывать мультимодальные архитектуры, такие как CLIP, Flamingo или модели, основанные на трансформерах.\n",
    "- Оценивать способность моделей извлекать осмысленную информацию из изображений и корректно отвечать на вопросы.\n",
    "- Тренировать модели для более глубокого семантического понимания объектов, действий, контекстов и отношений между элементами на изображении.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Помощники с искусственным интеллектом**\n",
    "VQA полезен для создания интерактивных приложений, способных понимать и интерпретировать визуальные сцены:\n",
    "- **Цифровые помощники**: Например, системы для описания и анализа фотографий, которые могут отвечать на вопросы, такие как «Что изображено на фотографии?» или «Кто на переднем плане?».\n",
    "- **Роботы и автономные системы**: Робот может использовать VQA для взаимодействия с человеком, понимая команды или вопросы, связанные с окружающей средой.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Доступность для людей с ограниченными возможностями**\n",
    "VQA поддерживает создание технологий для помощи людям с ограниченным зрением. Примеры:\n",
    "- **Приложения для слабовидящих**: Могут описывать сцены и отвечать на вопросы о них, например, «Сколько людей на фотографии?» или «Есть ли рядом дорога?».\n",
    "- **Инклюзивные интерфейсы**: Обеспечивают голосовую поддержку для работы с визуальными элементами.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Рекомендательные системы и поиск**\n",
    "Датасет используется для разработки систем, которые понимают визуальный контекст и текстовые запросы:\n",
    "- **Поиск по изображениям**: Улучшение поисковых систем, где запросы могут быть текстовыми, а результаты зависят от анализа изображений.\n",
    "- **Рекомендации в e-commerce**: Ответы на вопросы о продуктах, таких как «Какого цвета эта футболка?» или «Подходит ли эта сумка к черному платью?».\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Безопасность и наблюдение**\n",
    "VQA может быть полезен для анализа изображений в системах безопасности:\n",
    "- **Видеоаналитика**: Ответы на вопросы типа «Сколько людей находится в кадре?» или «Что делает человек на изображении?».\n",
    "- **Управление системами наблюдения**: Более точная обработка данных с камер для выявления событий или аномалий.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Образование и тренировка моделей**\n",
    "VQA используется для:\n",
    "- **Образовательных приложений**: Создание интерактивных материалов, которые могут отвечать на вопросы учащихся по изображениям, например, в биологии («Какая часть растения показана?») или географии («Какой это город?»).\n",
    "- **Исследований ИИ**: Разработка новых методов мультимодальной обработки данных, таких как совмещение текстовых и визуальных признаков.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Медицинская визуализация**\n",
    "Датасет может быть адаптирован для работы в медицинской сфере:\n",
    "- **Диагностические системы**: Анализ медицинских изображений с ответами на вопросы врачей, например, «Есть ли опухоль на снимке?» или «Какие изменения заметны?».\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Развлечения и игры**\n",
    "Применение в создании интерактивных игр, где игрок может задавать вопросы о виртуальной сцене:\n",
    "- **Игры с дополненной реальностью**: Взаимодействие с виртуальными объектами через вопросы и ответы.\n",
    "- **Кино и медиа**: Анализ сцен в фильмах или сериалов для автоматической генерации описаний.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Автономные транспортные средства**\n",
    "- **Навигация**: Ответы на вопросы типа «Что находится перед автомобилем?» или «Есть ли препятствия на дороге?».\n",
    "- **Контекстное понимание окружающей среды**: Анализ ситуации на дороге для принятия решений.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Научные исследования**\n",
    "- Анализ мультимодальных данных для изучения человеческого восприятия и понимания визуальной информации.\n",
    "- Исследование сложности вопросов, связанных с визуальными задачами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет Visual Question Answering (VQA) обладает множеством преимуществ, которые делают его важным инструментом для исследований и приложений в области мультимодального искусственного интеллекта.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Мультимодальность**\n",
    "- **Объединение текста и изображений**: VQA сочетает визуальные и текстовые данные, что позволяет моделям учиться работать с двумя типами информации одновременно.\n",
    "- **Разнообразие задач**: Датасет подходит для задач распознавания изображений, обработки текста, семантического анализа и их объединения.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Реалистичность данных**\n",
    "- **Естественные изображения**: Используются изображения из реального мира (например, из MS COCO), что делает задачи релевантными для реальных приложений.\n",
    "- **Естественные вопросы**: Вопросы и ответы, собранные от людей, имеют большую естественность и разнообразие, чем автоматически сгенерированные данные.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Разнообразие данных**\n",
    "- **Широкий спектр тем**: Включает вопросы о цветах, количестве объектов, действиях, взаимодействиях, местоположениях, текстурах и многом другом.\n",
    "- **Разные уровни сложности**: Датасет охватывает как простые вопросы (например, «Какого цвета яблоко?»), так и сложные, требующие рассуждений (например, «Что произойдет дальше?»).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Поддержка обучения и исследований**\n",
    "- **Универсальность**: Используется для обучения и тестирования мультимодальных моделей, таких как трансформеры или мультимодальные сетевые архитектуры.\n",
    "- **Тестирование генерализации**: Проверяет способность моделей отвечать на ранее неизвестные вопросы, основанные на новых изображениях.\n",
    "- **Обучение связей между объектами**: Модели могут изучать отношения между объектами, такие как близость, взаимодействие и взаимное расположение.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Воспроизводимость и стандартизация**\n",
    "- **Стандартный набор данных**: VQA является общеизвестным и широко используемым в сообществе искусственного интеллекта, что позволяет легко сравнивать результаты различных моделей.\n",
    "- **Четкие метрики оценки**: Метрики, такие как точность ответов, помогают объективно измерять производительность моделей.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Применимость в различных областях**\n",
    "- **Многоцелевое использование**: Датасет может быть использован в обучении моделей для анализа изображений, создания описаний и ответов на вопросы.\n",
    "- **Подготовка к реальным задачам**: Сценарии, заложенные в VQA, близки к реальным приложениям, таким как помощники с ИИ, медицинский анализ и системы наблюдения.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Простота использования**\n",
    "- **Легкий доступ**: Датасет хорошо документирован и интегрирован в исследовательские фреймворки, такие как ParlAI и PyTorch.\n",
    "- **Совместимость**: Поддерживает работу с современными инструментами машинного обучения, включая трансформеры, мультимодальные модели и библиотеки для работы с изображениями.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Вызовы для ИИ**\n",
    "- **Поддержка новых подходов**: Датасет мотивирует исследователей разрабатывать архитектуры, объединяющие текст и изображения.\n",
    "- **Сложные задачи**: Модели должны справляться с распознаванием объектов, логическим выводом и пониманием контекста, что поднимает планку развития ИИ.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Доступ к большому количеству данных**\n",
    "- **Обширность датасета**: Включает сотни тысяч вопросов и изображений, что предоставляет богатый материал для обучения моделей.\n",
    "- **Разнообразие форматов**: Вопросы представлены в текстовом формате, а изображения в виде реальных сцен, что позволяет изучать взаимодействие между ними.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Адаптивность**\n",
    "- **Адаптация к другим задачам**: Хотя датасет разработан для VQA, его можно адаптировать для создания описаний изображений, генерации текстов или других мультимодальных задач.\n",
    "- **Поддержка дообучения**: Модели, обученные на VQA, могут быть дообучены для более специализированных задач, таких как медицинская визуализация или интерактивные системы.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Стимулирование развития ИИ**\n",
    "- **Синергия технологий**: Датасет способствует интеграции подходов из областей компьютерного зрения, обработки текста и логических рассуждений.\n",
    "- **Тестирование новых идей**: Позволяет исследователям экспериментировать с различными архитектурами и подходами для мультимодальных задач.\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Способствует улучшению пользовательского опыта**\n",
    "- **Понимание естественного языка**: Разработка моделей, которые могут отвечать на вопросы, улучшает взаимодействие человека с ИИ.\n",
    "- **Доступ к информации**: Системы, основанные на VQA, могут помогать пользователям интерпретировать изображения и предоставлять информацию в удобной форме.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на широкие возможности, датасет Visual Question Answering (VQA) имеет ряд недостатков, которые следует учитывать при его использовании:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Ограниченность контекста**\n",
    "- **Отсутствие дополнительной информации**: Датасет опирается только на изображение и текст вопроса, что ограничивает модели в доступе к внешним знаниям, например, о мире, событиях или культурных особенностях.\n",
    "- **Недостаток мультимодального контекста**: Вопросы могут быть неоднозначными без дополнительного контекста, например, «Сколько здесь людей?» — если изображение частично обрезано, это вызывает проблемы интерпретации.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Качество аннотаций**\n",
    "- **Ошибки и несоответствия в аннотациях**: Часть вопросов или ответов может быть некорректной или не согласованной, например, неправильное количество объектов или ошибки в грамматике.\n",
    "- **Шум в данных**: Некоторые вопросы слишком общие или бессмысленные для анализа, такие как «Какого цвета это?» без явного объекта.\n",
    "- **Субъективность ответов**: На некоторые вопросы могут быть несколько корректных ответов, что затрудняет обучение моделей (например, для вопроса «Как выглядит этот человек?» можно ответить разными способами).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Ограниченность данных**\n",
    "- **Дисбаланс классов**: Частые ответы, такие как «да», «нет», «2», «красный», могут быть значительно переоценены в модели, а редкие ответы недостаточно представлены.\n",
    "- **Неравномерность сложности**: Некоторые вопросы требуют простого извлечения признаков, а другие требуют сложного рассуждения, что создает разрыв в уровне сложности.\n",
    "- **Ограниченность визуального содержания**: Датасет не охватывает все возможные визуальные сценарии, например, он плохо подходит для анализа медицинских изображений, научных данных или сложных графиков.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Узость лингвистической структуры**\n",
    "- **Ограниченность языков**: Датасет в основном на английском языке, что затрудняет его использование в мультиязычных приложениях.\n",
    "- **Повторяющиеся шаблоны вопросов**: Многие вопросы имеют одинаковую структуру, что упрощает задачу для моделей, но снижает их способность к генерализации на новые типы вопросов.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Отсутствие глубокой мультимодальности**\n",
    "- **Ограниченность текстовой информации**: Отсутствие метаданных, таких как описание сцены, или связанного текста (например, текст, написанный на изображении), которые могли бы улучшить понимание.\n",
    "- **Недостаточная связь с действиями**: Датасет больше ориентирован на статичные изображения, чем на динамические сцены, что ограничивает его применимость для анализа видео.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Проблемы с генерализацией**\n",
    "- **Слабая адаптация к реальным данным**: Вопросы в реальной жизни могут быть более сложными и менее структурированными, чем те, что представлены в датасете.\n",
    "- **Невозможность обработать неявные запросы**: Некоторые вопросы требуют логических выводов или внешнего знания, чего не предусмотрено в датасете.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Узкая направленность задач**\n",
    "- **Преобладание тривиальных вопросов**: Многие вопросы относятся к простым задачам, таким как подсчет объектов или определение цвета, что не отражает сложных мультимодальных задач.\n",
    "- **Мало вопросов высокого уровня**: Недостаточно задач, связанных с более сложным анализом, например, интерпретацией действий или связей между объектами.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Требования к вычислительным ресурсам**\n",
    "- **Большие вычислительные затраты**: Работа с VQA требует значительных ресурсов для обработки изображений и текстов, что делает его недоступным для малых исследовательских групп или на слабом оборудовании.\n",
    "- **Сложность мультимодальных моделей**: Совмещение обработки текстов и изображений увеличивает требования к архитектуре моделей и их обучению.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Ограниченная интерпретируемость**\n",
    "- **Сложность анализа ошибок**: Трудно понять, почему модель дала неверный ответ — из-за ошибки в текстовой части, визуальной интерпретации или несогласованности данных.\n",
    "- **Недостаток объяснений**: Датасет не предоставляет механизма для объяснения, почему был дан определенный ответ, что важно для доверия к системе.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Потенциальные этические проблемы**\n",
    "- **Стереотипы в данных**: В датасете могут быть зафиксированы культурные или социальные стереотипы, которые обученные модели могут повторять.\n",
    "- **Ошибки при использовании в критически важных областях**: Например, неправильные ответы в медицинской или юридической области могут привести к серьезным последствиям.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
