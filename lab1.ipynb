{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1 \"Градиентный спуск и его модификации\"\n",
    "\n",
    "## Задание\n",
    "\n",
    "1. Градиентный спуск и его модификации\n",
    "   - Выбрать [тестовые функции оптимизации](https://ru.wikipedia.org/wiki/Тестовые_функции_для_оптимизации) (2 шт)\n",
    "   - Запрограммировать собственную реализацию классического градиентного спуска\n",
    "   - Запрограммировать пайлайн тестирования алгоритма оптимизации\n",
    "     - Визуализации функции и точки оптимума\n",
    "     - Вычисление погрешности найденного решения в сравнение с аналитическим для нескольких запусков\n",
    "     - Визуализации точки найденного решения (можно добавить анимацию на плюс балл)\n",
    "   - Запрограммировать метод вычисления градиента\n",
    "     - Передача функции градиента от пользователя\n",
    "     - Символьное вычисление градиента (например с помощью [sympy](https://www.sympy.org/en/index.html)) (на доп балл)\n",
    "     - Численная аппроксимация градиента (на доп балл)\n",
    "   - Запрограммировать одну моментную модификацию и протестировать ее\n",
    "   - Запрограммировать одну адаптивную модификацию и протестировать ее\n",
    "   - Запрограммировать метод эфолюции темпа обучения и/или метод выбора начального приближения и протестировать их\n",
    "   - `Will be unclocked afetr Lecture №5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение\n",
    "\n",
    "В качестве тестовых функций возьмем: \n",
    "- **функция Розенброка** \n",
    "\n",
    "1. $ f(x) = \\sum _ {i=1} ^ {n-1} [100(x_{i+1}-x_{i}^{2})^2+(x_i-1)^2]$\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7e/Rosenbrock%27s_function_in_3D.pdf/page1-640px-Rosenbrock%27s_function_in_3D.pdf.jpg?uselang=ru)\n",
    "\n",
    "2. Минимум: $ f(1_0, 1_1, ..., 1_n)=0 $\n",
    "\n",
    "3. Метод поиска: $-\\infty \\leq x_i \\leq \\infty, 1 \\leq i \\leq n$\n",
    "\n",
    "- **функция Била**\n",
    "\n",
    "1. $ f(x,y) = (1.5-x+xy)^2+(2.25-x+xy^2)^2+(2.625-x+xy^3)^2 $\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Beale%27s_function.pdf/page1-640px-Beale%27s_function.pdf.jpg?uselang=ru)\n",
    "\n",
    "2. Минимум: $ f(3, 0.5) = 0 $\n",
    "\n",
    "3. Метод поиска: $-4.5 \\leq x,y \\leq 4.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вычисление выбранных тестовых функций. Для функции Розенброка реализуем вариант функции от двух переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(points):\n",
    "    \"\"\"\n",
    "    Функция Розенброка для массива точек [[x0, y0], [x1, y1], ...].\n",
    "    \n",
    "    Parameters:\n",
    "        points (np.ndarray): Массив размерности (N, 2), где каждая строка - это [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Значения функции Розенброка для каждой пары [x, y].\n",
    "    \"\"\"\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2\n",
    "\n",
    "def beale(points):\n",
    "    \"\"\"\n",
    "    Функция Била для массива точек [[x0, y0], [x1, y1], ...].\n",
    "\n",
    "    Parameters:\n",
    "        points (np.ndarray): Массив размерности (N, 2), где каждая строка - это [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Значения функции Била для каждой пары [x, y].\n",
    "    \"\"\"\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    return (\n",
    "        (1.5 - x + x * y) ** 2 +\n",
    "        (2.25 - x + x * y ** 2) ** 2 +\n",
    "        (2.625 - x + x * y ** 3) ** 2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем классический алгоритм градиентного спуска. На данном этапе будем подавать при вызове функцию для подсчета градиента. Позже, после выполнения задания по численной аппроксимации, сосздадим еще одну имплементацию, которая уже будет использовать приблежнные значения градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(func, grad_func, initial_points, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Классический градиентный спуск для оптимизации функции.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): Целевая функция, которую нужно минимизировать.\n",
    "        grad_func (callable): Функция, возвращающая градиенты целевой функции.\n",
    "        initial_points (np.ndarray): Начальные точки размерности (N, 2).\n",
    "        learning_rate (float): Скорость обучения (шаг градиентного спуска).\n",
    "        max_iter (int): Максимальное количество итераций.\n",
    "        tol (float): Критерий остановки по норме градиента.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (np.ndarray, list): Оптимизированные точки размерности (N, 2) и список траекторий точек.\n",
    "    \"\"\"\n",
    "    points = initial_points.copy()\n",
    "    trajectory = [initial_points.copy()]\n",
    "    for i in range(max_iter):\n",
    "        gradients = grad_func(points)\n",
    "        norm = np.linalg.norm(gradients, axis=1).max()\n",
    "        if norm < tol:\n",
    "            break\n",
    "        points -= learning_rate * gradients\n",
    "        trajectory.append(points.copy())\n",
    "    return points, trajectory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку метод градиентного спуска реализован пока что через подсчет по готовой функции градиента, воспользуемся библиотекой sympy для нахождения функции градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(func):\n",
    "    # Запрос функции от пользователя\n",
    "    expr_str = func\n",
    "    \n",
    "    # Парсим введенную строку как выражение sympy\n",
    "    expr = sp.sympify(expr_str)\n",
    "    \n",
    "    # Определяем переменные\n",
    "    variables = list(expr.free_symbols)  # Автоматически находим все переменные\n",
    "    \n",
    "    # Вычисляем градиент\n",
    "    gradient = [sp.diff(expr, var) for var in variables]\n",
    "    \n",
    "    # Вывод результата\n",
    "    for var, partial_derivative in zip(variables, gradient):\n",
    "        print(f\"∂/∂{var}: {partial_derivative}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции Розенброка:\n",
      "∂/∂y: -200*x**2 + 200*y\n",
      "∂/∂x: -400*x*(-x**2 + y) + 2*x - 2\n",
      "Градиент функции Била:\n",
      "∂/∂y: 15.75*x*y**2*(0.380952380952381*x*y**3 - 0.380952380952381*x + 1) + 9.0*x*y*(0.444444444444444*x*y**2 - 0.444444444444444*x + 1) + 3.0*x*(0.666666666666667*x*y - 0.666666666666667*x + 1)\n",
      "∂/∂x: 2.25*(1.33333333333333*y - 1.33333333333333)*(0.666666666666667*x*y - 0.666666666666667*x + 1) + 5.0625*(0.888888888888889*y**2 - 0.888888888888889)*(0.444444444444444*x*y**2 - 0.444444444444444*x + 1) + 6.890625*(0.761904761904762*y**3 - 0.761904761904762)*(0.380952380952381*x*y**3 - 0.380952380952381*x + 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Градиент функции Розенброка:\")\n",
    "compute_gradient('100 * (y - x**2)**2 + (x - 1)**2')\n",
    "print(\"Градиент функции Била:\")\n",
    "compute_gradient('((1.5 - x + x * y)**2 + (2.25 - x + x * y**2)**2 + (2.625 - x + x * y**3)**2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем полученные градиенты в функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_gradient(points):\n",
    "    \"\"\"\n",
    "    Градиенты функции Розенброка для массива точек [[x0, y0], [x1, y1], ...].\n",
    "\n",
    "    Parameters:\n",
    "        points (np.ndarray): Массив размерности (N, 2), где каждая строка - это [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Градиенты функции Розенброка для каждой пары [x, y].\n",
    "    \"\"\"\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    grad_x = -2 * (1 - x) - 400 * x * (y - x ** 2)\n",
    "    grad_y = 200 * (y - x ** 2)\n",
    "    return np.stack([grad_x, grad_y], axis=1)\n",
    "\n",
    "def beale_gradient(points):\n",
    "    \"\"\"\n",
    "    Градиенты функции Била для массива точек [[x0, y0], [x1, y1], ...].\n",
    "\n",
    "    Parameters:\n",
    "        points (np.ndarray): Массив размерности (N, 2), где каждая строка - это [x, y].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Градиенты функции Била для каждой пары [x, y].\n",
    "    \"\"\"\n",
    "    x = points[:, 0]\n",
    "    y = points[:, 1]\n",
    "    grad_x = (\n",
    "        2.25 * (1.33333333333333 * y - 1.33333333333333) * (0.666666666666667 * x * y - 0.666666666666667 * x + 1) +\n",
    "        5.0625 * (0.888888888888889 * y ** 2 - 0.888888888888889) * (0.444444444444444 * x * y ** 2 - 0.444444444444444 * x + 1) +\n",
    "        6.890625 * (0.761904761904762 * y ** 3 - 0.761904761904762) * (0.380952380952381 * x * y ** 3 - 0.380952380952381 * x + 1)\n",
    "    )\n",
    "    grad_y = (\n",
    "        15.75 * x * y ** 2 * (0.380952380952381 * x * y ** 3 - 0.380952380952381 * x + 1) +\n",
    "        9.0 * x * y * (0.444444444444444 * x * y ** 2 - 0.444444444444444 * x + 1) +\n",
    "        3.0 * x * (0.666666666666667 * x * y - 0.666666666666667 * x + 1)\n",
    "    )\n",
    "    return np.stack([grad_x, grad_y], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем пайплайн визуализации и подсчета ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d(func, trajectories, x_range, y_range, title=\"3D Visualization\"):\n",
    "    \"\"\"\n",
    "    Визуализация функции и траектории градиентного спуска в 3D с использованием Plotly.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): Целевая функция для визуализации.\n",
    "        trajectory (list): Список точек траектории [[x0, y0], [x1, y1], ...].\n",
    "        x_range (tuple): Диапазон значений для оси X (min, max).\n",
    "        y_range (tuple): Диапазон значений для оси Y (min, max).\n",
    "        title (str): Заголовок графика.\n",
    "    \"\"\"\n",
    "    x = np.linspace(x_range[0], x_range[1], 100)\n",
    "    y = np.linspace(y_range[0], y_range[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = func(np.column_stack([X.ravel(), Y.ravel()])).reshape(X.shape)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Поверхность функции\n",
    "    fig.add_trace(go.Surface(z=Z, x=X, y=Y, colorscale='Viridis', opacity=0.7))\n",
    "    \n",
    "    for i, trajectory in enumerate(trajectories):\n",
    "        trajectory = np.array(trajectory).reshape(len(trajectory), 2)\n",
    "        # Траектория спуска\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=trajectory[:, 0],\n",
    "            y=trajectory[:, 1],\n",
    "            z=func(trajectory),\n",
    "            mode='lines+markers',\n",
    "            marker=dict(size=5, color=colors.qualitative.Plotly[i]),\n",
    "            line=dict(color=colors.qualitative.Plotly[i], width=2)\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title=title, scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z'\n",
    "    ))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию вычисления погрешности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(func, optimized_points, true_minimum):\n",
    "    \"\"\"\n",
    "    Вычисляет погрешность между найденными минимумами и аналитическим минимумом.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): Целевая функция.\n",
    "        optimized_points (np.ndarray): Найденные точки минимума размерности (N, 2).\n",
    "        true_minimum (np.ndarray): Аналитический минимум (1, 2).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Среднеквадратичная ошибка координат и значений функции.\n",
    "    \"\"\"\n",
    "    errors_points = np.linalg.norm(optimized_points - true_minimum, axis=1)\n",
    "    errors_values = func(optimized_points) - func(true_minimum)\n",
    "    return np.mean(errors_points ** 2), np.mean(errors_values ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим работу алгоритма для функции Розенброка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = []\n",
    "result = []\n",
    "points = [[0.0, 3.0], [2.0, -1.0], [-2.0, -1.0]]\n",
    "for point in points:\n",
    "    # Пример использования градиентного спуска для функции Розенброка\n",
    "    initial_points = np.array([point])\n",
    "    optimized_points, trajectory = gradient_descent(\n",
    "        func=rosenbrock,\n",
    "        grad_func=rosenbrock_gradient,\n",
    "        initial_points=initial_points,\n",
    "        learning_rate=0.001,\n",
    "        max_iter=10000,\n",
    "        tol=1e-8\n",
    "    )\n",
    "    trajectories += [trajectory]\n",
    "    result += [optimized_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d(\n",
    "    func=rosenbrock,\n",
    "    trajectories=trajectories,\n",
    "    x_range=(-2, 2),\n",
    "    y_range=(-1, 3),\n",
    "    title=\"Функция Розенброка\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Поскольку github не отображает анимированный plotly, я сохранил изображение*\n",
    "\n",
    "![rosenbrock](./data/rosenbrock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для координат минимума: 0.00013828855649871994\n",
      "MSE для найденного минимума: 1.2448700002868934e-09\n"
     ]
    }
   ],
   "source": [
    "errors_points, errors_values = compute_error(rosenbrock, np.array(result).reshape(len(result), 2), np.array([[1.0, 1.0]]))\n",
    "print(f\"MSE для координат минимума: {errors_points}\")\n",
    "print(f\"MSE для найденного минимума: {errors_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим работу алгоритма для функции Била"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = []\n",
    "result = []\n",
    "points = [[3.0, 3.0], [3.0, -3.0]]\n",
    "for point in points:\n",
    "    # Пример использования градиентного спуска для функции Била\n",
    "    initial_points = np.array([point])\n",
    "    optimized_points, trajectory = gradient_descent(\n",
    "        func=beale,\n",
    "        grad_func=beale_gradient,\n",
    "        initial_points=initial_points,\n",
    "        learning_rate=0.00001,\n",
    "        max_iter=100000,\n",
    "        tol=1e-8\n",
    "    )\n",
    "    trajectories += [trajectory]\n",
    "    result += [optimized_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_3d(\n",
    "    func=beale,\n",
    "    trajectories=trajectories,\n",
    "    x_range=(-3, 3),\n",
    "    y_range=(-3, 3),\n",
    "    title=\"Функция Била\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Поскольку github не отображает анимированный plotly, я сохранил изображение*\n",
    "\n",
    "![rosenbrock](./data/beale.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для координат минимума: 0.14163942064273022\n",
      "MSE для найденного минимума: 0.0018572580734444357\n"
     ]
    }
   ],
   "source": [
    "errors_points, errors_values = compute_error(beale, np.array(result).reshape(len(result), 2), np.array([[3.0, 0.5]]))\n",
    "print(f\"MSE для координат минимума: {errors_points}\")\n",
    "print(f\"MSE для найденного минимума: {errors_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Высокая ошибка для функции Била объясняется долгой сходимостью. Если поставить больше итераций, то ошибка будет меньше. На визуализации видно, что спуск идет ровно в минимум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуем моментную и адаптивную модификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_gradient_descent(func, grad_func, initial_points, learning_rate=0.01, momentum=0.9, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с моментумом для оптимизации функции.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): Целевая функция, которую нужно минимизировать.\n",
    "        grad_func (callable): Функция, возвращающая градиенты целевой функции.\n",
    "        initial_points (np.ndarray): Начальные точки размерности (N, 2).\n",
    "        learning_rate (float): Скорость обучения (шаг градиентного спуска).\n",
    "        momentum (float): Коэффициент момента.\n",
    "        max_iter (int): Максимальное количество итераций.\n",
    "        tol (float): Критерий остановки по норме градиента.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (np.ndarray, list): Оптимизированные точки размерности (N, 2) и список траекторий точек.\n",
    "    \"\"\"\n",
    "    points = initial_points.copy()\n",
    "    velocity = np.zeros_like(points)\n",
    "    trajectory = [points.copy()]\n",
    "    for i in range(max_iter):\n",
    "        gradients = grad_func(points)\n",
    "        norm = np.linalg.norm(gradients, axis=1).max()\n",
    "        if norm < tol:\n",
    "            break\n",
    "        velocity = momentum * velocity - learning_rate * gradients\n",
    "        points += velocity\n",
    "        trajectory.append(points.copy())\n",
    "    return points, trajectory\n",
    "\n",
    "def adaptive_gradient_descent(func, grad_func, initial_points, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Адаптивный градиентный спуск для оптимизации функции.\n",
    "\n",
    "    Parameters:\n",
    "        func (callable): Целевая функция, которую нужно минимизировать.\n",
    "        grad_func (callable): Функция, возвращающая градиенты целевой функции.\n",
    "        initial_points (np.ndarray): Начальные точки размерности (N, 2).\n",
    "        learning_rate (float): Начальная скорость обучения.\n",
    "        max_iter (int): Максимальное количество итераций.\n",
    "        tol (float): Критерий остановки по норме градиента.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (np.ndarray, list): Оптимизированные точки размерности (N, 2) и список траекторий точек.\n",
    "    \"\"\"\n",
    "    points = initial_points.copy()\n",
    "    cache = np.zeros_like(points)\n",
    "    trajectory = [points.copy()]\n",
    "    epsilon = 1e-8\n",
    "    for i in range(max_iter):\n",
    "        gradients = grad_func(points)\n",
    "        norm = np.linalg.norm(gradients, axis=1).max()\n",
    "        if norm < tol:\n",
    "            break\n",
    "        cache += gradients ** 2\n",
    "        adjusted_gradients = gradients / (np.sqrt(cache) + epsilon)\n",
    "        points -= learning_rate * adjusted_gradients\n",
    "        trajectory.append(points.copy())\n",
    "    return points, trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
